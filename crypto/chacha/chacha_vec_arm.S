#
# Copyright (c) 2016, Kurt Cancemi (kurt@x64architecture.com)
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#

# This file contains a pre-compiled version of chacha_vec.c for ARM. This is
# needed to support using NEON at runtime without it being a prerequsite.
#
# This file was generated by chacha_vec_arm_generate using the following
# compiler command:
#     arm-linux-gnueabihf-gcc -DARM_ASM -O3 -mfpu=neon -fpic -I"../../include" -S chacha_vec.c -o chacha_vec.s

#if !defined(OPENSSL_NO_ASM)

	.arch armv6
#if defined(__ARM_PCS_VFP)
	.eabi_attribute 28, 1
#else
	.eabi_attribute 28, 0
#endif
	.fpu neon
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 2
	.eabi_attribute 30, 2
	.eabi_attribute 34, 1
	.eabi_attribute 18, 4
	.arm
	.syntax divided
	.file	"chacha_vec.c"
	.text
	.align	2
	.global	CRYPTO_chacha_20_neon
	.type	CRYPTO_chacha_20_neon, %function
CRYPTO_chacha_20_neon:
	@ args = 8, pretend = 0, frame = 144
	@ frame_needed = 1, uses_anonymous_args = 0
	stmfd	sp!, {r4, r5, r6, r7, r8, r9, r10, fp, lr}
	vpush.64	{d8, d9, d10, d11, d12, d13}
	mov	r4, r3
	mov	r8, r2
	mov	r9, r0
	add	fp, sp, #80
	sub	sp, sp, #148
	ldr	lr, [fp, #4]
	sub	sp, sp, #96
	add	ip, sp, #15
	str	r0, [fp, #-212]
	str	r1, [fp, #-208]
	ldr	r0, [lr]	@ unaligned
	str	r2, [fp, #-216]
	str	r3, [fp, #-188]
	ldr	r2, [lr, #8]	@ unaligned
	mov	r3, ip, lsr #4
	mov	r7, r1
	ldr	r1, [lr, #4]	@ unaligned
	mov	ip, r3
	str	r3, [fp, #-224]
	sub	r3, fp, #96
	mov	ip, ip, asl #4
	stmia	r3!, {r0, r1, r2}
	add	r5, ip, #64
	ldr	r1, [r4, #4]	@ unaligned
	ldr	r2, [r4, #8]	@ unaligned
	ldr	r3, [r4, #12]	@ unaligned
	ldr	r0, [r4]	@ unaligned
	mov	r6, ip
	str	ip, [fp, #-160]
	stmia	r5!, {r0, r1, r2, r3}
	mov	r2, r8
	ldr	r0, [r4, #16]!	@ unaligned
	ldr	ip, .L94+36
	add	r1, r6, #64
	mov	r5, r6
	ldr	r3, [r4, #12]	@ unaligned
	mov	r6, r1
	str	r1, [fp, #-168]
	umull	r2, ip, ip, r2
	vldr	d26, [r5, #64]
	vldr	d27, [r5, #72]
	ldr	r1, [r4, #4]	@ unaligned
	ldr	r2, [r4, #8]	@ unaligned
	mov	r4, r6
	ldr	r6, [lr]
	str	r6, [fp, #-176]
	stmia	r4!, {r0, r1, r2, r3}
	movs	r3, ip, lsr #7
	ldr	r2, [lr, #4]
	ldr	r1, [fp, #8]
	mov	r4, r2
	str	r2, [fp, #-180]
	ldr	r2, [lr, #8]
	vldr	d28, [r5, #64]
	vldr	d29, [r5, #72]
	mov	lr, r2
	str	r2, [fp, #-184]
	ldr	r2, .L94+16
	str	r6, [fp, #-112]
	str	r1, [fp, #-116]
	str	r4, [fp, #-108]
	str	lr, [fp, #-104]
.LPIC24:
	add	r2, pc, r2
	vldr	d22, [fp, #-116]
	vldr	d23, [fp, #-108]
	vld1.32	{q12}, [r2]
	beq	.L2
	add	r3, r3, #1
	add	r2, r1, #2
	add	r3, r3, r3, lsl #1
	add	r1, r7, #192
	mov	r3, r3, asl #6
	str	r3, [fp, #-220]
	add	r3, r7, r3
	str	r3, [fp, #-204]
	add	r3, r9, #192
	str	r2, [fp, #-172]
	str	r1, [fp, #-164]
	str	r3, [fp, #-192]
	vldr	d6, .L94
	vldr	d7, .L94+8
.L4:
	ldr	r3, [fp, #-188]
	ldr	r4, [fp, #-176]
	ldr	r10, .L94+20
	ldr	ip, [r3, #12]
	ldr	r2, [r3]
	str	ip, [fp, #-124]
	ldr	ip, [r3, #16]
	mov	r0, r2
	ldr	lr, [r3, #24]
	ldr	r2, [r3, #4]
	ldr	r1, [r3, #8]
	mov	r7, ip
	ldr	ip, [r3, #20]
	ldr	r3, [r3, #28]
	str	lr, [fp, #-140]
	str	r3, [fp, #-144]
	ldr	r3, .L94+32
	ldr	lr, [fp, #-180]
	str	r3, [fp, #-136]
	ldr	r3, .L94+28
	str	ip, [fp, #-148]
	mov	r8, r3
	ldr	ip, [fp, #-172]
	ldr	r3, [fp, #-184]
	str	r2, [fp, #-120]
	ldr	r9, .L94+24
	mov	r5, lr
	mov	lr, r4
	mov	r4, #10
	str	r4, [fp, #-152]
	ldr	r2, [fp, #-124]
	mov	r4, r3
	mov	r3, ip
	mov	ip, r0
	ldr	r0, [fp, #-120]
	mov	r6, r10
	mov	r10, r8
	mov	r8, r9
	vadd.i32	q2, q3, q11
	vmov	q15, q14  @ v4si
	vmov	q5, q13  @ v4si
	vmov	q0, q12  @ v4si
	vmov	q1, q11  @ v4si
	vmov	q8, q14  @ v4si
	vmov	q6, q13  @ v4si
	vmov	q4, q12  @ v4si
	b	.L95
.L96:
	.align	3
.L94:
	.word	1
	.word	0
	.word	0
	.word	0
	.word	.LANCHOR0-(.LPIC24+8)
	.word	1634760805
	.word	857760878
	.word	2036477234
	.word	1797285236
	.word	-1431655765
.L95:
.L3:
	add	r9, r8, r0
	ldr	r0, [fp, #-136]
	add	r8, r10, r1
	add	r10, r0, r2
	mov	r0, r10
	eor	lr, lr, r9
	str	r0, [fp, #-136]
	eor	r4, r4, r0
	ldr	r0, [fp, #-148]
	add	r6, ip, r6
	eor	r5, r5, r8
	mov	lr, lr, ror #16
	mov	r10, r6
	eor	r3, r3, r6
	add	r6, lr, r0
	ldr	r0, [fp, #-140]
	mov	r5, r5, ror #16
	add	r0, r5, r0
	str	r0, [fp, #-124]
	ldr	r0, [fp, #-144]
	mov	r4, r4, ror #16
	add	r0, r4, r0
	str	r0, [fp, #-128]
	mov	r3, r3, ror #16
	ldr	r0, [fp, #-120]
	add	r7, r3, r7
	eor	ip, ip, r7
	eor	r0, r0, r6
	str	r6, [fp, #-132]
	mov	ip, ip, ror #20
	ldr	r6, [fp, #-124]
	mov	r0, r0, ror #20
	add	r10, r10, ip
	add	r9, r9, r0
	eor	r1, r1, r6
	eor	r3, r3, r10
	eor	lr, lr, r9
	mov	r3, r3, ror #24
	mov	r1, r1, ror #20
	mov	lr, lr, ror #24
	str	lr, [fp, #-120]
	add	r8, r8, r1
	add	r7, r7, r3
	ldr	r6, [fp, #-128]
	str	r8, [fp, #-140]
	eor	r5, r5, r8
	mov	lr, r3
	ldr	r8, [fp, #-120]
	mov	r3, r7
	ldr	r7, [fp, #-132]
	eor	r2, r2, r6
	add	r7, r7, r8
	ldr	r8, [fp, #-124]
	ldr	r6, [fp, #-136]
	mov	r5, r5, ror #24
	eor	r0, r0, r7
	str	r9, [fp, #-136]
	mov	r2, r2, ror #20
	add	r9, r8, r5
	add	r6, r6, r2
	eor	r1, r1, r9
	mov	r0, r0, ror #25
	ldr	r8, [fp, #-128]
	eor	r4, r4, r6
	str	r3, [fp, #-128]
	eor	ip, ip, r3
	add	r3, r10, r0
	ldr	r10, [fp, #-136]
	mov	r1, r1, ror #25
	add	r10, r10, r1
	mov	r4, r4, ror #24
	str	r9, [fp, #-124]
	add	r8, r8, r4
	mov	r9, r3
	mov	r3, r10
	eor	r2, r2, r8
	eor	lr, lr, r3
	ldr	r10, [fp, #-140]
	str	r3, [fp, #-144]
	mov	r3, lr
	mov	r2, r2, ror #25
	ldr	lr, [fp, #-120]
	add	r10, r10, r2
	mov	r3, r3, ror #16
	add	r8, r8, r3
	eor	lr, lr, r10
	mov	ip, ip, ror #25
	str	r9, [fp, #-140]
	eor	r4, r4, r9
	ldr	r9, [fp, #-124]
	str	r8, [fp, #-124]
	ldr	r8, [fp, #-128]
	add	r6, ip, r6
	mov	lr, lr, ror #16
	add	r8, r8, lr
	eor	r5, r5, r6
	str	r8, [fp, #-132]
	ldr	r8, [fp, #-124]
	mov	r5, r5, ror #16
	add	r7, r7, r5
	eor	r1, r1, r8
	str	r3, [fp, #-120]
	ldr	r8, [fp, #-132]
	eor	ip, ip, r7
	eor	r2, r2, r8
	mov	ip, ip, ror #20
	add	r6, r6, ip
	mov	r2, r2, ror #20
	ldr	r8, [fp, #-144]
	mov	r3, r1, ror #20
	mov	r1, r2
	mov	r2, r6
	mov	r4, r4, ror #16
	str	r3, [fp, #-128]
	add	r8, r8, r3
	eor	r5, r5, r2
	ldr	r3, [fp, #-120]
	add	r9, r9, r4
	eor	r0, r0, r9
	eor	r3, r3, r8
	mov	r5, r5, ror #24
	add	r2, r7, r5
	str	r6, [fp, #-136]
	ldr	r7, [fp, #-124]
	ldr	r6, [fp, #-140]
	mov	r0, r0, ror #20
	mov	r3, r3, ror #24
	add	r7, r7, r3
	add	r6, r6, r0
	str	r1, [fp, #-156]
	add	r10, r10, r1
	eor	r4, r4, r6
	mov	r1, r7
	eor	ip, ip, r2
	eor	lr, lr, r10
	str	r2, [fp, #-148]
	mov	r2, r1
	ldr	r1, [fp, #-128]
	mov	r4, r4, ror #24
	str	r7, [fp, #-144]
	ldr	r7, [fp, #-132]
	add	r9, r9, r4
	eor	r1, r1, r2
	mov	lr, lr, ror #24
	ldr	r2, [fp, #-156]
	eor	r0, r0, r9
	str	r9, [fp, #-140]
	add	r7, r7, lr
	ldr	r9, [fp, #-152]
	eor	r2, r2, r7
	subs	r9, r9, #1
	mov	r0, r0, ror #25
	vadd.i32	q4, q4, q6
	vadd.i32	q0, q0, q5
	veor	q1, q4, q1
	veor	q2, q0, q2
	vrev32.16	q1, q1
	vrev32.16	q2, q2
	vadd.i32	q8, q8, q1
	vadd.i32	q15, q15, q2
	veor	q6, q8, q6
	veor	q9, q15, q5
	vshl.i32	q10, q6, #12
	vshl.i32	q5, q9, #12
	vsri.32	q10, q6, #20
	vsri.32	q5, q9, #20
	vadd.i32	q4, q4, q10
	vadd.i32	q0, q0, q5
	veor	q1, q4, q1
	veor	q2, q0, q2
	vshl.i32	q6, q1, #8
	vshl.i32	q9, q2, #8
	vsri.32	q6, q1, #24
	vsri.32	q9, q2, #24
	vadd.i32	q8, q6, q8
	vadd.i32	q15, q9, q15
	veor	q10, q8, q10
	veor	q5, q15, q5
	vshl.i32	q1, q10, #7
	vext.32	q6, q6, q6, #3
	vsri.32	q1, q10, #25
	vext.32	q8, q8, q8, #2
	vext.32	q1, q1, q1, #1
	vshl.i32	q10, q5, #7
	vadd.i32	q4, q4, q1
	vsri.32	q10, q5, #25
	veor	q6, q6, q4
	vext.32	q10, q10, q10, #1
	vrev32.16	q6, q6
	vadd.i32	q0, q0, q10
	vadd.i32	q8, q8, q6
	vext.32	q9, q9, q9, #3
	veor	q1, q8, q1
	veor	q9, q9, q0
	vshl.i32	q2, q1, #12
	vrev32.16	q9, q9
	vsri.32	q2, q1, #20
	vext.32	q15, q15, q15, #2
	vadd.i32	q4, q4, q2
	vadd.i32	q15, q15, q9
	veor	q6, q4, q6
	veor	q10, q15, q10
	vshl.i32	q1, q6, #8
	vshl.i32	q5, q10, #12
	vsri.32	q1, q6, #24
	vsri.32	q5, q10, #20
	vadd.i32	q8, q1, q8
	vadd.i32	q0, q0, q5
	veor	q2, q8, q2
	veor	q9, q0, q9
	vshl.i32	q6, q2, #7
	str	r9, [fp, #-152]
	vsri.32	q6, q2, #25
	mov	ip, ip, ror #25
	vshl.i32	q2, q9, #8
	str	r0, [fp, #-120]
	vsri.32	q2, q9, #24
	mov	r1, r1, ror #25
	vadd.i32	q15, q2, q15
	mov	r2, r2, ror #25
	veor	q10, q15, q5
	vext.32	q1, q1, q1, #1
	vshl.i32	q5, q10, #7
	vext.32	q8, q8, q8, #2
	vsri.32	q5, q10, #25
	vext.32	q6, q6, q6, #3
	vext.32	q2, q2, q2, #1
	vext.32	q15, q15, q15, #2
	vext.32	q5, q5, q5, #3
	bne	.L3
	str	ip, [fp, #-196]
	mov	ip, r3
	ldr	r3, [fp, #-164]
	str	r7, [fp, #-156]
	ldr	r7, [fp, #-168]
	ldr	r0, [r3, #-192]!	@ unaligned
	str	r1, [fp, #-132]
	str	r2, [fp, #-124]
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	mov	r9, r8
	str	lr, [fp, #-152]
	mov	r8, r4
	mov	lr, r7
	mov	r4, r7
	str	r10, [fp, #-128]
	stmia	r4!, {r0, r1, r2, r3}
	mov	r10, r6
	ldr	r3, [fp, #-160]
	ldr	r6, [fp, #-192]
	vldr	d20, [r3, #64]
	vldr	d21, [r3, #72]
	vadd.i32	q4, q4, q12
	veor	q10, q10, q4
	vstr	d20, [r3, #64]
	vstr	d21, [r3, #72]
	ldmia	lr!, {r0, r1, r2, r3}
	sub	r4, r6, #192
	ldr	lr, [fp, #-164]
	vadd.i32	q6, q13, q6
	str	r0, [r6, #-192]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r2, [r4, #8]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [lr, #-176]!	@ unaligned
	mov	r4, r7
	vadd.i32	q8, q14, q8
	ldr	r1, [lr, #4]	@ unaligned
	ldr	r2, [lr, #8]	@ unaligned
	ldr	r3, [lr, #12]	@ unaligned
	mov	lr, r7
	vadd.i32	q1, q11, q1
	stmia	r4!, {r0, r1, r2, r3}
	mov	r4, r7
	ldr	r3, [fp, #-160]
	vldr	d20, [r3, #64]
	vldr	d21, [r3, #72]
	veor	q6, q10, q6
	vstr	d12, [r3, #64]
	vstr	d13, [r3, #72]
	ldmia	lr!, {r0, r1, r2, r3}
	vadd.i32	q0, q0, q12
	ldr	lr, [fp, #-164]
	vadd.i32	q5, q13, q5
	str	r0, [r6, #-176]	@ unaligned
	str	r1, [r6, #-172]	@ unaligned
	str	r2, [r6, #-168]	@ unaligned
	str	r3, [r6, #-164]	@ unaligned
	ldr	r0, [lr, #-160]!	@ unaligned
	vadd.i32	q15, q14, q15
	vadd.i32	q11, q3, q11
	ldr	r1, [lr, #4]	@ unaligned
	ldr	r2, [lr, #8]	@ unaligned
	ldr	r3, [lr, #12]	@ unaligned
	mov	lr, r7
	vadd.i32	q2, q11, q2
	stmia	r4!, {r0, r1, r2, r3}
	vadd.i32	q11, q3, q11
	ldr	r3, [fp, #-160]
	vldr	d20, [r3, #64]
	vldr	d21, [r3, #72]
	veor	q8, q10, q8
	vstr	d16, [r3, #64]
	vstr	d17, [r3, #72]
	ldmia	lr!, {r0, r1, r2, r3}
	vadd.i32	q11, q11, q3
	ldr	lr, [fp, #-164]
	ldr	r7, [fp, #-172]
	str	r1, [r6, #-156]	@ unaligned
	str	r2, [r6, #-152]	@ unaligned
	str	r0, [r6, #-160]	@ unaligned
	str	r3, [r6, #-148]	@ unaligned
	ldr	r2, [fp, #-152]
	ldr	r0, [lr, #-144]!	@ unaligned
	ldr	r1, [fp, #-176]
	ldr	r4, [fp, #-168]
	add	r1, r1, r2
	add	ip, ip, r7
	ldr	r2, [lr, #8]	@ unaligned
	ldr	r3, [lr, #12]	@ unaligned
	str	ip, [fp, #-172]
	str	r1, [fp, #-152]
	mov	ip, r4
	ldr	r1, [lr, #4]	@ unaligned
	mov	lr, r4
	ldr	r4, [fp, #-180]
	stmia	ip!, {r0, r1, r2, r3}
	add	r4, r4, r5
	ldr	r3, [fp, #-160]
	str	r4, [fp, #-192]
	vldr	d20, [r3, #64]
	vldr	d21, [r3, #72]
	ldr	r4, [fp, #-164]
	veor	q1, q10, q1
	vstr	d2, [r3, #64]
	vstr	d3, [r3, #72]
	ldmia	lr!, {r0, r1, r2, r3}
	mov	ip, r4
	ldr	r5, [fp, #-184]
	add	r5, r5, r8
	str	r0, [r6, #-144]	@ unaligned
	str	r5, [fp, #-200]
	str	r1, [r6, #-140]	@ unaligned
	str	r2, [r6, #-136]	@ unaligned
	str	r3, [r6, #-132]	@ unaligned
	ldr	r0, [ip, #-128]!	@ unaligned
	ldr	r5, [fp, #-168]
	mov	r8, r4
	ldr	r1, [ip, #4]	@ unaligned
	ldr	r2, [ip, #8]	@ unaligned
	ldr	r3, [ip, #12]	@ unaligned
	mov	lr, r5
	mov	ip, r5
	stmia	lr!, {r0, r1, r2, r3}
	mov	lr, r4
	ldr	r3, [fp, #-160]
	vldr	d18, [r3, #64]
	vldr	d19, [r3, #72]
	veor	q0, q9, q0
	vstr	d0, [r3, #64]
	vstr	d1, [r3, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	mov	r4, r5
	mov	ip, r5
	str	r0, [r6, #-128]	@ unaligned
	str	r1, [r6, #-124]	@ unaligned
	str	r2, [r6, #-120]	@ unaligned
	str	r3, [r6, #-116]	@ unaligned
	ldr	r0, [lr, #-112]!	@ unaligned
	ldr	r1, [lr, #4]	@ unaligned
	ldr	r2, [lr, #8]	@ unaligned
	ldr	r3, [lr, #12]	@ unaligned
	mov	lr, r5
	stmia	r4!, {r0, r1, r2, r3}
	mov	r4, r5
	ldr	r3, [fp, #-160]
	vldr	d18, [r3, #64]
	vldr	d19, [r3, #72]
	veor	q5, q9, q5
	vstr	d10, [r3, #64]
	vstr	d11, [r3, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	mov	ip, r8
	str	r0, [r6, #-112]	@ unaligned
	str	r1, [r6, #-108]	@ unaligned
	str	r2, [r6, #-104]	@ unaligned
	str	r3, [r6, #-100]	@ unaligned
	ldr	r0, [ip, #-96]!	@ unaligned
	ldr	r1, [ip, #4]	@ unaligned
	ldr	r2, [ip, #8]	@ unaligned
	ldr	r3, [ip, #12]	@ unaligned
	mov	ip, r5
	stmia	r4!, {r0, r1, r2, r3}
	mov	r4, r8
	ldr	r3, [fp, #-160]
	vldr	d16, [r3, #64]
	vldr	d17, [r3, #72]
	veor	q8, q8, q15
	vstr	d16, [r3, #64]
	vstr	d17, [r3, #72]
	ldmia	lr!, {r0, r1, r2, r3}
	mov	lr, r5
	str	r0, [r6, #-96]	@ unaligned
	str	r1, [r6, #-92]	@ unaligned
	str	r2, [r6, #-88]	@ unaligned
	str	r3, [r6, #-84]	@ unaligned
	ldr	r0, [r4, #-80]!	@ unaligned
	ldr	r1, [r4, #4]	@ unaligned
	ldr	r2, [r4, #8]	@ unaligned
	ldr	r3, [r4, #12]	@ unaligned
	stmia	lr!, {r0, r1, r2, r3}
	mov	lr, r8
	ldr	r3, [fp, #-160]
	vldr	d16, [r3, #64]
	vldr	d17, [r3, #72]
	veor	q8, q8, q2
	vstr	d16, [r3, #64]
	vstr	d17, [r3, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	ldr	ip, .L94+20
	str	r0, [r6, #-80]	@ unaligned
	str	r1, [r6, #-76]	@ unaligned
	str	r2, [r6, #-72]	@ unaligned
	str	r3, [r6, #-68]	@ unaligned
	ldr	r3, [r8, #-64]
	add	ip, r10, ip
	eor	ip, ip, r3
	ldr	r3, .L94+24
	str	ip, [r6, #-64]
	ldr	r2, [r8, #-60]
	add	r3, r9, r3
	eor	r2, r2, r3
	ldr	r0, [fp, #-128]
	ldr	r3, .L94+28
	str	r2, [r6, #-60]
	ldr	r2, [r8, #-56]
	add	r3, r0, r3
	eor	r2, r2, r3
	ldr	r0, [fp, #-136]
	ldr	r3, .L94+32
	str	r2, [r6, #-56]
	add	r3, r0, r3
	ldr	r2, [r8, #-52]
	ldr	r0, [fp, #-188]
	eor	r3, r3, r2
	str	r3, [r6, #-52]
	ldr	r2, [r8, #-48]
	ldr	r3, [r0]
	ldr	r8, [fp, #-196]
	mov	r1, r6
	add	r3, r8, r3
	eor	r3, r3, r2
	str	r3, [r6, #-48]
	ldr	r8, [fp, #-120]
	ldr	r3, [r0, #4]
	ldr	r2, [lr, #-44]
	add	r3, r8, r3
	eor	r3, r3, r2
	str	r3, [r6, #-44]
	ldr	r3, [r0, #8]
	ldr	r8, [fp, #-132]
	ldr	r2, [lr, #-40]
	add	r3, r8, r3
	eor	r3, r3, r2
	str	r3, [r6, #-40]
	ldr	r8, [fp, #-124]
	ldr	r3, [r0, #12]
	ldr	r2, [lr, #-36]
	add	r3, r8, r3
	eor	r3, r3, r2
	str	r3, [r6, #-36]
	ldr	ip, [fp, #-156]
	ldr	r2, [r0, #16]
	ldr	r3, [lr, #-32]
	add	r6, ip, r2
	eor	r6, r6, r3
	str	r6, [r1, #-32]
	ldr	ip, [fp, #-148]
	ldr	r2, [r0, #20]
	ldr	r3, [lr, #-28]
	add	r5, ip, r2
	eor	r5, r5, r3
	str	r5, [r1, #-28]
	mov	r6, r1
	ldr	r4, [r0, #24]
	mov	r1, r0
	ldr	r0, [fp, #-140]
	ldr	r3, [lr, #-24]
	add	r4, r0, r4
	mov	r2, lr
	eor	r4, r4, r3
	str	r4, [r6, #-24]
	ldr	r0, [fp, #-144]
	ldr	lr, [r1, #28]
	ldr	r3, [r2, #-20]
	add	lr, r0, lr
	eor	lr, lr, r3
	str	lr, [r6, #-20]
	mov	lr, r2
	ldr	ip, [r2, #-16]
	ldr	r2, [fp, #-172]
	ldr	r1, [fp, #-152]
	eor	ip, ip, r2
	str	ip, [r6, #-16]
	ldr	r0, [lr, #-12]
	add	r2, r7, #3
	eor	r0, r0, r1
	str	r0, [r6, #-12]
	str	r2, [fp, #-172]
	ldr	r1, [lr, #-8]
	ldr	r4, [fp, #-192]
	ldr	r5, [fp, #-200]
	eor	r1, r1, r4
	str	r1, [r6, #-8]
	ldr	r0, [fp, #-204]
	ldr	r2, [lr, #-4]
	add	r1, lr, #192
	eor	r2, r2, r5
	add	r3, r6, #192
	cmp	r1, r0
	str	r1, [fp, #-164]
	str	r2, [r6, #-4]
	str	r3, [fp, #-192]
	bne	.L4
	ldr	r3, [fp, #-220]
	ldr	r2, [fp, #-208]
	sub	r3, r3, #192
	add	r2, r2, r3
	str	r2, [fp, #-208]
	ldr	r2, [fp, #-212]
	add	r3, r2, r3
	str	r3, [fp, #-212]
.L2:
	ldr	r3, [fp, #-216]
	ldr	r7, .L94+36
	umull	r2, r7, r7, r3
	mov	r7, r7, lsr #7
	add	r7, r7, r7, lsl #1
	sub	r7, r3, r7, asl #6
	movs	r7, r7, lsr #6
	beq	.L5
	ldr	r3, [fp, #-208]
	ldr	r4, [fp, #-160]
	add	lr, r3, #16
	ldr	r3, [fp, #-212]
	ldr	r5, [fp, #-168]
	add	ip, r3, #16
	mov	r6, r7
	vldr	d30, .L97
	vldr	d31, .L97+8
.L7:
	mov	r3, #10
	vmov	q2, q11  @ v4si
	vmov	q8, q14  @ v4si
	vmov	q9, q13  @ v4si
	vmov	q10, q12  @ v4si
.L6:
	subs	r3, r3, #1
	vadd.i32	q10, q9, q10
	veor	q3, q10, q2
	vrev32.16	q3, q3
	vadd.i32	q8, q8, q3
	veor	q9, q8, q9
	vshl.i32	q2, q9, #12
	vsri.32	q2, q9, #20
	vadd.i32	q10, q10, q2
	veor	q3, q10, q3
	vshl.i32	q9, q3, #8
	vsri.32	q9, q3, #24
	vadd.i32	q8, q9, q8
	vext.32	q9, q9, q9, #3
	veor	q2, q8, q2
	vext.32	q8, q8, q8, #2
	vshl.i32	q3, q2, #7
	vsri.32	q3, q2, #25
	vext.32	q3, q3, q3, #1
	vadd.i32	q10, q10, q3
	veor	q9, q9, q10
	vrev32.16	q9, q9
	vadd.i32	q8, q8, q9
	veor	q3, q8, q3
	vshl.i32	q2, q3, #12
	vsri.32	q2, q3, #20
	vmov	q3, q2  @ v4si
	vadd.i32	q10, q10, q2
	veor	q9, q10, q9
	vshl.i32	q2, q9, #8
	vsri.32	q2, q9, #24
	vadd.i32	q8, q2, q8
	vext.32	q2, q2, q2, #1
	veor	q3, q8, q3
	vext.32	q8, q8, q8, #2
	vshl.i32	q9, q3, #7
	vsri.32	q9, q3, #25
	vext.32	q9, q9, q9, #3
	bne	.L6
	mov	r3, lr
	mov	r9, r5
	ldr	r0, [r3, #-16]!	@ unaligned
	mov	r8, r5
	vadd.i32	q3, q10, q12
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	vadd.i32	q9, q13, q9
	mov	r10, r5
	stmia	r9!, {r0, r1, r2, r3}
	vldr	d20, [r4, #64]
	vldr	d21, [r4, #72]
	veor	q10, q10, q3
	vstr	d20, [r4, #64]
	vstr	d21, [r4, #72]
	ldmia	r8!, {r0, r1, r2, r3}
	mov	r9, r5
	mov	r8, r5
	vadd.i32	q8, q14, q8
	str	r0, [ip, #-16]	@ unaligned
	str	r1, [ip, #-12]	@ unaligned
	str	r2, [ip, #-8]	@ unaligned
	str	r3, [ip, #-4]	@ unaligned
	ldr	r0, [lr]	@ unaligned
	ldr	r1, [lr, #4]	@ unaligned
	ldr	r2, [lr, #8]	@ unaligned
	ldr	r3, [lr, #12]	@ unaligned
	vadd.i32	q3, q2, q11
	subs	r6, r6, #1
	stmia	r9!, {r0, r1, r2, r3}
	vldr	d20, [r4, #64]
	vldr	d21, [r4, #72]
	veor	q10, q10, q9
	vstr	d20, [r4, #64]
	vstr	d21, [r4, #72]
	ldmia	r8!, {r0, r1, r2, r3}
	mov	r9, lr
	mov	r8, r5
	vadd.i32	q11, q11, q15
	str	r0, [ip]	@ unaligned
	str	r1, [ip, #4]	@ unaligned
	str	r2, [ip, #8]	@ unaligned
	str	r3, [ip, #12]	@ unaligned
	ldr	r0, [r9, #16]!	@ unaligned
	add	ip, ip, #64
	ldr	r1, [r9, #4]	@ unaligned
	ldr	r2, [r9, #8]	@ unaligned
	ldr	r3, [r9, #12]	@ unaligned
	mov	r9, r5
	stmia	r10!, {r0, r1, r2, r3}
	vldr	d18, [r4, #64]
	vldr	d19, [r4, #72]
	veor	q9, q9, q8
	vstr	d18, [r4, #64]
	vstr	d19, [r4, #72]
	ldmia	r8!, {r0, r1, r2, r3}
	mov	r10, lr
	mov	r8, r5
	add	lr, lr, #64
	str	r0, [ip, #-48]	@ unaligned
	str	r1, [ip, #-44]	@ unaligned
	str	r2, [ip, #-40]	@ unaligned
	str	r3, [ip, #-36]	@ unaligned
	ldr	r0, [r10, #32]!	@ unaligned
	ldr	r1, [r10, #4]	@ unaligned
	ldr	r2, [r10, #8]	@ unaligned
	ldr	r3, [r10, #12]	@ unaligned
	stmia	r9!, {r0, r1, r2, r3}
	vldr	d16, [r4, #64]
	vldr	d17, [r4, #72]
	veor	q8, q8, q3
	vstr	d16, [r4, #64]
	vstr	d17, [r4, #72]
	ldmia	r8!, {r0, r1, r2, r3}
	str	r0, [ip, #-32]	@ unaligned
	str	r1, [ip, #-28]	@ unaligned
	str	r2, [ip, #-24]	@ unaligned
	str	r3, [ip, #-20]	@ unaligned
	bne	.L7
	ldr	r3, [fp, #-208]
	mov	r7, r7, asl #6
	add	r3, r3, r7
	str	r3, [fp, #-208]
	ldr	r3, [fp, #-212]
	add	r3, r3, r7
	str	r3, [fp, #-212]
	ldr	r3, [fp, #-216]
.L5:
	ands	lr, r3, #63
	beq	.L1
	mov	r3, #10
	vmov	q3, q11  @ v4si
	vmov	q8, q14  @ v4si
	vmov	q15, q13  @ v4si
	vmov	q10, q12  @ v4si
.L9:
	subs	r3, r3, #1
	vadd.i32	q10, q15, q10
	veor	q9, q10, q3
	vrev32.16	q9, q9
	vadd.i32	q8, q8, q9
	veor	q15, q8, q15
	vshl.i32	q3, q15, #12
	vsri.32	q3, q15, #20
	vadd.i32	q10, q10, q3
	veor	q15, q10, q9
	vshl.i32	q9, q15, #8
	vsri.32	q9, q15, #24
	vadd.i32	q8, q9, q8
	vext.32	q9, q9, q9, #3
	veor	q3, q8, q3
	vext.32	q8, q8, q8, #2
	vshl.i32	q15, q3, #7
	vsri.32	q15, q3, #25
	vext.32	q15, q15, q15, #1
	vadd.i32	q10, q10, q15
	veor	q9, q9, q10
	vrev32.16	q9, q9
	vadd.i32	q8, q8, q9
	veor	q15, q8, q15
	vshl.i32	q3, q15, #12
	vsri.32	q3, q15, #20
	vmov	q15, q3  @ v4si
	vadd.i32	q10, q10, q3
	veor	q9, q10, q9
	vshl.i32	q3, q9, #8
	vsri.32	q3, q9, #24
	vadd.i32	q8, q3, q8
	vext.32	q3, q3, q3, #1
	veor	q9, q8, q15
	vext.32	q8, q8, q8, #2
	vshl.i32	q15, q9, #7
	vsri.32	q15, q9, #25
	vext.32	q15, q15, q15, #3
	bne	.L9
	cmp	lr, #15
	bhi	.L91
	vadd.i32	q10, q10, q12
	ldr	r3, [fp, #-160]
	vst1.64	{d20-d21}, [r3:128]
	b	.L98
.L99:
	.align	3
.L97:
	.word	1
	.word	0
	.word	0
	.word	0
.L98:
.L13:
	ldr	r3, [fp, #-216]
	and	r3, r3, #48
	cmp	lr, r3
	bls	.L1
	ldr	r6, [fp, #-208]
	ldr	r7, [fp, #-212]
	add	r1, r3, #16
	add	r2, r6, r1
	add	r0, r6, r3
	add	r1, r7, r1
	add	ip, r7, r3
	cmp	r0, r1
	cmpcc	ip, r2
	movcs	r2, #1
	movcc	r2, #0
	rsb	r4, r3, lr
	cmp	r4, #18
	movls	r2, #0
	andhi	r2, r2, #1
	cmp	r2, #0
	beq	.L15
	and	r1, r0, #7
	rsb	r1, r1, #0
	and	r1, r1, #15
	cmp	r1, r4
	movcs	r1, r4
	cmp	r1, #0
	moveq	r2, r3
	beq	.L16
	ldr	r8, [fp, #-224]
	ldrb	r0, [r0]	@ zero_extendqisi2
	cmp	r1, #1
	ldrb	r5, [r3, r8, asl #4]	@ zero_extendqisi2
	add	r2, r3, #1
	eor	r0, r0, r5
	strb	r0, [ip]
	beq	.L16
	ldrb	r0, [r6, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #2
	mov	r5, r6
	eor	r0, r0, ip
	strb	r0, [r7, r2]
	add	r2, r3, #2
	beq	.L16
	ldrb	r0, [r6, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #3
	eor	r0, r0, ip
	strb	r0, [r7, r2]
	add	r2, r3, #3
	beq	.L16
	ldrb	r0, [r6, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #4
	mov	r6, r7
	eor	r0, r0, ip
	strb	r0, [r7, r2]
	add	r2, r3, #4
	beq	.L16
	ldrb	r0, [r5, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #5
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #5
	beq	.L16
	ldrb	r0, [r5, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #6
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #6
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #7
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #7
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #8
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #8
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #9
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #9
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #10
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #10
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #11
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #11
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #12
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #12
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #13
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #13
	beq	.L16
	ldrb	ip, [r5, r2]	@ zero_extendqisi2
	ldrb	r0, [r2, r8, asl #4]	@ zero_extendqisi2
	cmp	r1, #15
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #14
	bne	.L16
	ldrb	r0, [r5, r2]	@ zero_extendqisi2
	ldrb	ip, [r2, r8, asl #4]	@ zero_extendqisi2
	eor	r0, r0, ip
	strb	r0, [r6, r2]
	add	r2, r3, #15
.L16:
	rsb	ip, r1, r4
	sub	r5, ip, #16
	sub	r0, r4, #1
	mov	r4, r5, lsr #4
	rsb	r0, r1, r0
	add	r4, r4, #1
	cmp	r0, #14
	mov	r0, r4, asl #4
	bls	.L18
	add	r1, r3, r1
	ldr	r5, [fp, #-208]
	ldr	r3, [fp, #-160]
	ldr	r6, [fp, #-212]
	cmp	r4, #1
	add	r3, r3, r1
	add	r5, r5, r1
	vld1.8	{q8}, [r3]
	vld1.64	{d18-d19}, [r5:64]
	add	r1, r6, r1
	veor	q8, q8, q9
	vst1.8	{q8}, [r1]
	beq	.L19
	cmp	r4, #2
	add	r6, r3, #16
	vldr	d18, [r5, #16]
	vldr	d19, [r5, #24]
	vld1.8	{q8}, [r6]
	add	r6, r1, #16
	veor	q8, q8, q9
	vst1.8	{q8}, [r6]
	beq	.L19
	cmp	r4, #3
	vldr	d18, [r5, #32]
	vldr	d19, [r5, #40]
	add	r4, r3, #32
	vld1.8	{q8}, [r4]
	add	r4, r1, #32
	veor	q8, q8, q9
	vst1.8	{q8}, [r4]
	beq	.L19
	vldr	d18, [r5, #48]
	vldr	d19, [r5, #56]
	add	r3, r3, #48
	add	r1, r1, #48
	vld1.8	{q8}, [r3]
	veor	q8, q8, q9
	vst1.8	{q8}, [r1]
.L19:
	cmp	r0, ip
	add	r2, r2, r0
	beq	.L1
.L18:
	ldr	r4, [fp, #-224]
	ldr	r5, [fp, #-208]
	ldr	r6, [fp, #-212]
	ldrb	r0, [r2, r4, asl #4]	@ zero_extendqisi2
	ldrb	r1, [r5, r2]	@ zero_extendqisi2
	add	r3, r2, #1
	cmp	r3, lr
	eor	r1, r1, r0
	strb	r1, [r6, r2]
	bcs	.L1
	ldrb	ip, [r5, r3]	@ zero_extendqisi2
	ldrb	r0, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #2
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #3
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r0, [r5, r3]	@ zero_extendqisi2
	ldrb	ip, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #4
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #5
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r0, [r5, r3]	@ zero_extendqisi2
	ldrb	ip, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #6
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #7
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r0, [r5, r3]	@ zero_extendqisi2
	ldrb	ip, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #8
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #9
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r0, [r5, r3]	@ zero_extendqisi2
	ldrb	ip, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #10
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #11
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r0, [r5, r3]	@ zero_extendqisi2
	ldrb	ip, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r1, r2, #12
	cmp	lr, r1
	eor	r0, r0, ip
	strb	r0, [r6, r3]
	bls	.L1
	ldrb	r0, [r5, r1]	@ zero_extendqisi2
	ldrb	ip, [r1, r4, asl #4]	@ zero_extendqisi2
	add	r3, r2, #13
	cmp	lr, r3
	eor	r0, r0, ip
	strb	r0, [r6, r1]
	bls	.L1
	ldrb	r1, [r5, r3]	@ zero_extendqisi2
	ldrb	r0, [r3, r4, asl #4]	@ zero_extendqisi2
	add	r2, r2, #14
	cmp	lr, r2
	eor	r1, r1, r0
	strb	r1, [r6, r3]
	bls	.L1
	ldr	r3, [fp, #-208]
	ldrb	r1, [r2, r4, asl #4]	@ zero_extendqisi2
	ldrb	r3, [r3, r2]	@ zero_extendqisi2
	eor	r3, r3, r1
	ldr	r1, [fp, #-212]
	strb	r3, [r1, r2]
.L1:
	sub	sp, fp, #80
	@ sp needed
	vldm	sp!, {d8-d13}
	ldmfd	sp!, {r4, r5, r6, r7, r8, r9, r10, fp, pc}
.L91:
	ldr	r3, [fp, #-208]
	ldr	ip, [fp, #-168]
	vadd.i32	q10, q10, q12
	ldr	r0, [r3]	@ unaligned
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	mov	r4, ip
	cmp	lr, #31
	stmia	r4!, {r0, r1, r2, r3}
	ldr	r3, [fp, #-160]
	vldr	d18, [r3, #64]
	vldr	d19, [r3, #72]
	veor	q9, q9, q10
	vstr	d18, [r3, #64]
	vstr	d19, [r3, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	ldr	ip, [fp, #-212]
	str	r0, [ip]	@ unaligned
	str	r1, [ip, #4]	@ unaligned
	str	r2, [ip, #8]	@ unaligned
	str	r3, [ip, #12]	@ unaligned
	bhi	.L92
	vadd.i32	q13, q13, q15
	ldr	r3, [fp, #-160]
	vstr	d26, [r3, #16]
	vstr	d27, [r3, #24]
	b	.L13
.L15:
	ldr	r2, [fp, #-160]
	add	r3, r2, r3
	ldr	r2, [fp, #-208]
	add	lr, r2, lr
.L23:
	ldrb	r2, [r0], #1	@ zero_extendqisi2
	ldrb	r1, [r3], #1	@ zero_extendqisi2
	cmp	r0, lr
	eor	r2, r2, r1
	strb	r2, [ip], #1
	bne	.L23
	sub	sp, fp, #80
	@ sp needed
	vldm	sp!, {d8-d13}
	ldmfd	sp!, {r4, r5, r6, r7, r8, r9, r10, fp, pc}
.L92:
	ldr	r3, [fp, #-208]
	ldr	ip, [fp, #-168]
	vadd.i32	q13, q13, q15
	ldr	r0, [r3, #16]!	@ unaligned
	mov	r4, ip
	cmp	lr, #47
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	stmia	r4!, {r0, r1, r2, r3}
	ldr	r4, [fp, #-212]
	ldr	r3, [fp, #-160]
	vldr	d18, [r3, #64]
	vldr	d19, [r3, #72]
	veor	q13, q9, q13
	vstr	d26, [r3, #64]
	vstr	d27, [r3, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	add	ip, r4, #16
	str	r0, [r4, #16]	@ unaligned
	str	r1, [ip, #4]	@ unaligned
	str	r2, [ip, #8]	@ unaligned
	str	r3, [ip, #12]	@ unaligned
	bhi	.L93
	vadd.i32	q8, q14, q8
	ldr	r3, [fp, #-160]
	vstr	d16, [r3, #32]
	vstr	d17, [r3, #40]
	b	.L13
.L93:
	ldr	r3, [fp, #-208]
	ldr	ip, [fp, #-168]
	ldr	r5, [fp, #-160]
	ldr	r0, [r3, #32]!	@ unaligned
	mov	r4, ip
	vadd.i32	q9, q3, q11
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	vadd.i32	q8, q14, q8
	vstr	d18, [r5, #48]
	vstr	d19, [r5, #56]
	stmia	ip!, {r0, r1, r2, r3}
	vldr	d18, [r5, #64]
	vldr	d19, [r5, #72]
	veor	q9, q9, q8
	vstr	d18, [r5, #64]
	vstr	d19, [r5, #72]
	ldmia	r4!, {r0, r1, r2, r3}
	ldr	r4, [fp, #-212]
	add	ip, r4, #32
	str	r0, [r4, #32]	@ unaligned
	str	r1, [ip, #4]	@ unaligned
	str	r2, [ip, #8]	@ unaligned
	str	r3, [ip, #12]	@ unaligned
	b	.L13
	.size	CRYPTO_chacha_20_neon, .-CRYPTO_chacha_20_neon
	.section	.rodata
	.align	2
.LANCHOR0 = . + 0
.LC0:
	.word	1634760805
	.word	857760878
	.word	2036477234
	.word	1797285236
	.ident	"GCC: (GNU) 5.3.0"
	.section	.note.GNU-stack,"",%progbits
#endif  /* !OPENSSL_NO_ASM */
